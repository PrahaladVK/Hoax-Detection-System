{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Install Dependencies**"
      ],
      "metadata": {
        "id": "cHxuYsKMZr1X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "289aiKAyZnGC"
      },
      "outputs": [],
      "source": [
        "!pip install -q requests beautifulsoup4 textblob nltk transformers torch gradio pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import Libraries**"
      ],
      "metadata": {
        "id": "i7mmOOhWZxiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, requests\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from transformers import pipeline\n",
        "import nltk\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "except Exception:\n",
        "    userdata = None\n",
        "\n",
        "for pkg in [\"punkt\", \"punkt_tab\", \"averaged_perceptron_tagger\", \"wordnet\", \"stopwords\"]:\n",
        "    try:\n",
        "        nltk.download(pkg, quiet=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Note: could not download {pkg}: {e}\")\n",
        "\n",
        "print(\"Imports ready and NLTK data prepared\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGcCiny8Zxvy",
        "outputId": "cbff880f-5967-4135-87b4-5d718fd31dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports ready and NLTK data prepared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Secret Key Management**"
      ],
      "metadata": {
        "id": "oQOc_x63Z4jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_api_key(key_name):\n",
        "    try:\n",
        "        if userdata is None:\n",
        "            return None\n",
        "        return userdata.get(key_name)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "SERPER_API_KEY = get_api_key('SERPER_API_KEY')\n"
      ],
      "metadata": {
        "id": "vr40rwOLZ425"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **All Classes(ES,AIAnalyzer,WebVerifier,Main Fn)**"
      ],
      "metadata": {
        "id": "Wtm2KhnibIKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, requests\n",
        "from collections import Counter\n",
        "\n",
        "class ExpertSystem:\n",
        "\n",
        "    def __init__(self):\n",
        "        # 1. Try to fetch clickbait word list from an online gist\n",
        "        try:\n",
        "            url = \"https://gist.githubusercontent.com/amitness/0a2ddbcb61c34eab04bad5a17fd8c86b/raw/clickbait.csv\"\n",
        "            text = requests.get(url, timeout=10).text\n",
        "            words = [\n",
        "                w for line in text.splitlines()\n",
        "                for w in line.lower().split()\n",
        "                if w.isalpha() and len(w) > 3\n",
        "            ]\n",
        "            self.clickbait_words = [w for w, _ in Counter(words).most_common(300)]\n",
        "            print(f\"Loaded {len(self.clickbait_words)} online clickbait words\")\n",
        "        except Exception as e:\n",
        "            # Fallback set if online fetch fails\n",
        "            self.clickbait_words = [\n",
        "                \"shocking\",\"unbelievable\",\"you won't believe\",\"secret\",\"exposed\",\n",
        "                \"revealed\",\"doctors hate\",\"this one trick\",\"breaking\",\"urgent\",\"alert\",\n",
        "                \"must see\",\"incredible\",\"miracle\",\"cure\",\"breakthrough\",\"proven\",\n",
        "                \"scientists claim\",\"researchers say\",\"study finds\",\"new study\",\n",
        "                \"amazing discovery\",\"medical wonder\"\n",
        "            ]\n",
        "            print(f\"Using fallback clickbait list ({len(self.clickbait_words)} words). Error: {e}\")\n",
        "\n",
        "        # 2. Define secondary heuristic lists\n",
        "        self.extreme_words = [\n",
        "            \"always\",\"never\",\"all\",\"none\",\"everyone\",\"nobody\",\"everywhere\",\"nowhere\",\n",
        "            \"completely\",\"totally\",\"absolutely\",\"entirely\",\"guaranteed\",\"100%\",\"undeniable\"\n",
        "        ]\n",
        "\n",
        "        self.emotional_words = [\n",
        "            \"outrage\",\"shocking\",\"devastating\",\"horrific\",\"amazing\",\"incredible\",\n",
        "            \"unbelievable\",\"stunning\",\"mind-blowing\",\"breakthrough\",\"miracle\",\n",
        "            \"revolutionary\",\"groundbreaking\"\n",
        "        ]\n",
        "\n",
        "        # 3. Common myth claims\n",
        "        self.consensus_myths = [\n",
        "            r\"\\bsun\\s+orbits\\s+the\\s+earth\\b\",\n",
        "            r\"\\bearth\\s+is\\s+flat\\b\",\n",
        "            r\"\\b5g\\s+causes\\s+covid\\b\",\n",
        "            r\"\\bmoon\\s+landing\\s+(is|was)\\s+(fake|a\\s+hoax)\\b\",\n",
        "        ]\n",
        "\n",
        "    def analyze(self, text):\n",
        "        t = (text or \"\").strip()\n",
        "        tl = t.lower()\n",
        "        score = 90\n",
        "        flags = []\n",
        "\n",
        "        # Consensus myth detection\n",
        "        for pat in self.consensus_myths:\n",
        "            if re.search(pat, tl):\n",
        "                score -= 40\n",
        "                flags.append(\"Contradicts established scientific consensus\")\n",
        "                break\n",
        "\n",
        "        # Clickbait / sensational language\n",
        "        c_count = sum(1 for w in self.clickbait_words if w in tl)\n",
        "        if c_count > 2:\n",
        "            score -= 20; flags.append(f\"High clickbait language ({c_count} instances)\")\n",
        "        elif c_count > 0:\n",
        "            score -= 10; flags.append(f\"Some clickbait or sensational terms ({c_count} instances)\")\n",
        "\n",
        "        # Extreme / absolute language\n",
        "        x_count = sum(1 for w in self.extreme_words if w in tl)\n",
        "        if x_count > 5:\n",
        "            score -= 15; flags.append(f\"Excessive extreme/absolute terms ({x_count} instances)\")\n",
        "        elif x_count > 0:\n",
        "            score -= 5; flags.append(f\"Some extreme or absolute terms ({x_count} instances)\")\n",
        "\n",
        "        # Emotional tone\n",
        "        e_count = sum(1 for w in self.emotional_words if w in tl)\n",
        "        if e_count > 3:\n",
        "            score -= 15; flags.append(f\"Highly emotional tone ({e_count} instances)\")\n",
        "        elif e_count > 0:\n",
        "            score -= 5; flags.append(f\"Some emotional tone ({e_count} instances)\")\n",
        "\n",
        "        # Suspicious numeric percentages\n",
        "        if re.search(r\"\\b\\d{2,}%\\b\", tl):\n",
        "            score -= 10; flags.append(\"Suspicious numeric claim (percentage)\")\n",
        "\n",
        "        # Hedging / rumor wording\n",
        "        if re.search(r\"\\b(reportedly|rumor|anonymous source|insider|leaked)\\b\", tl):\n",
        "            score -= 5; flags.append(\"Hedging/rumor wording without clear attribution\")\n",
        "\n",
        "        # Lack of sourcing\n",
        "        if not re.search(r\"(according to|source|study|research|report|journal|paper|data|doi:|https?://)\", tl):\n",
        "            score -= 10; flags.append(\"No explicit sources or citations found\")\n",
        "\n",
        "        # Exclamation / capitalization penalties\n",
        "        if text.count(\"!\") > 3:\n",
        "            score -= 10; flags.append(\"Excessive exclamation marks\")\n",
        "        caps_ratio = sum(1 for c in text if c.isupper()) / max(1, len(text))\n",
        "        if caps_ratio > 0.15:\n",
        "            score -= 10; flags.append(\"Excessive capitalization\")\n",
        "\n",
        "        return max(0, min(100, score)), flags\n",
        "\n",
        "\n",
        "class AIAnalyzer:\n",
        "    def __init__(self):\n",
        "        print(\"Loading AI models...\")\n",
        "        try:\n",
        "            self.sentiment_analyzer = pipeline(\n",
        "                \"sentiment-analysis\",\n",
        "                model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "            )\n",
        "            print(\"Sentiment analyzer loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load sentiment analyzer: {e}\")\n",
        "            self.sentiment_analyzer = None\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        t = (text or \"\")[:512]\n",
        "        if not t:\n",
        "            return {\"label\": \"NEUTRAL\", \"score\": 0.5}\n",
        "        if self.sentiment_analyzer is None:\n",
        "            blob = TextBlob(t)\n",
        "            p = blob.sentiment.polarity\n",
        "            if p > 0.1:  return {\"label\": \"POSITIVE\", \"score\": (p + 1) / 2}\n",
        "            if p < -0.1: return {\"label\": \"NEGATIVE\", \"score\": (1 - p) / 2}\n",
        "            return {\"label\": \"NEUTRAL\", \"score\": 0.5}\n",
        "        return self.sentiment_analyzer(t)[0]\n",
        "\n",
        "    def analyze_writing_quality(self, text):\n",
        "        t = text or \"\"\n",
        "        blob = TextBlob(t)\n",
        "\n",
        "        sentences = blob.sentences\n",
        "        words = blob.words\n",
        "\n",
        "        avg_sentence_length = (len(words) / max(1, len(sentences))) if sentences else 0\n",
        "\n",
        "        # Cap to 250 words to keep batch runs fast\n",
        "        words_for_spell = words[:250]\n",
        "        misspelled = sum(1 for w in words_for_spell if w.correct() != w)\n",
        "        spelling_score = 100 if not words_for_spell else max(0, 100 - (misspelled / len(words_for_spell) * 100))\n",
        "\n",
        "        return {\n",
        "            \"avg_sentence_length\": avg_sentence_length,\n",
        "            \"spelling_score\": spelling_score,\n",
        "            \"subjectivity\": blob.sentiment.subjectivity\n",
        "        }\n",
        "\n",
        "\n",
        "class WebVerifier:\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "        self.search_url = \"https://google.serper.dev/search\"\n",
        "\n",
        "    def search_news(self, query):\n",
        "        if not self.api_key:\n",
        "            return None\n",
        "        headers = {'X-API-KEY': self.api_key, 'Content-Type': 'application/json'}\n",
        "        payload = json.dumps({\"q\": query, \"num\": 5})\n",
        "        try:\n",
        "            r = requests.post(self.search_url, headers=headers, data=payload, timeout=10)\n",
        "            if r.status_code == 200:\n",
        "                return r.json()\n",
        "            else:\n",
        "                print(f\"Search API error: {r.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Search error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def verify_claim(self, news_text):\n",
        "        blob = TextBlob(news_text or \"\")\n",
        "        query = str(blob.sentences[0]) if blob.sentences else (news_text or \"\")[:100]\n",
        "        search_results = self.search_news(query)\n",
        "        if not search_results:\n",
        "            return {\"sources_found\": 0, \"reputable_sources\": 0, \"credibility_boost\": 0, \"sources\": []}\n",
        "\n",
        "        organic = search_results.get(\"organic\", [])\n",
        "        reputable_domains = [\n",
        "            \"reuters.com\",\"apnews.com\",\"bbc.com\",\"cnn.com\",\"nytimes.com\",\n",
        "            \"washingtonpost.com\",\"theguardian.com\",\"npr.org\",\"bloomberg.com\",\n",
        "            \"forbes.com\",\"wsj.com\",\"aljazeera.com\",\"scientificamerican.com\",\n",
        "            \"nature.com\",\"science.org\"\n",
        "        ]\n",
        "\n",
        "        sources, rep = [], 0\n",
        "        for res in organic[:5]:\n",
        "            link = res.get(\"link\",\"\")\n",
        "            title = res.get(\"title\",\"\")\n",
        "            snippet = res.get(\"snippet\",\"\")\n",
        "            is_rep = any(d in link for d in reputable_domains)\n",
        "            rep += int(is_rep)\n",
        "            sources.append({\"title\": title, \"url\": link, \"snippet\": snippet, \"is_reputable\": is_rep})\n",
        "\n",
        "        boost = 20 if rep >= 3 else 15 if rep >= 2 else 10 if rep >= 1 else (5 if len(sources) >= 3 else 0)\n",
        "        return {\"sources_found\": len(sources), \"reputable_sources\": rep, \"credibility_boost\": boost, \"sources\": sources}\n",
        "\n",
        "\n",
        "class HoaxDetector:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.expert_system = ExpertSystem()\n",
        "        self.ai_analyzer = AIAnalyzer()\n",
        "        self.web_verifier = WebVerifier(SERPER_API_KEY) if SERPER_API_KEY else None\n",
        "        self.strict_mode = True\n",
        "\n",
        "    def analyze(self, news_text, quiet: bool = False):\n",
        "        t = (news_text or \"\")\n",
        "        tl = t.lower()\n",
        "\n",
        "        # 1) Expert rules\n",
        "        expert_score, expert_flags = self.expert_system.analyze(t)\n",
        "\n",
        "        # 2) AI analysis\n",
        "        sentiment = self.ai_analyzer.analyze_sentiment(t)\n",
        "        writing_quality = self.ai_analyzer.analyze_writing_quality(t)\n",
        "\n",
        "        ai_adj = 0\n",
        "        if writing_quality.get(\"spelling_score\", 100) < 70: ai_adj -= 10\n",
        "        if writing_quality.get(\"subjectivity\", 0) > 0.7:   ai_adj -= 10\n",
        "\n",
        "        # 3) Web verification\n",
        "        verification_result = None\n",
        "        web_adj = 0\n",
        "        if self.web_verifier:\n",
        "            verification_result = self.web_verifier.verify_claim(t)\n",
        "            web_adj = verification_result.get(\"credibility_boost\", 0)\n",
        "        else:\n",
        "            verification_result = {\n",
        "                \"sources_found\": 0, \"reputable_sources\": 0, \"credibility_boost\": 0, \"sources\": [],\n",
        "                \"note\": \"Web verification unavailable (SERPER_API_KEY not configured).\"\n",
        "            }\n",
        "\n",
        "        # 4) Final score + strict caps\n",
        "        final_score = max(0, min(100, expert_score + ai_adj + web_adj))\n",
        "\n",
        "        no_web = (self.web_verifier is None)\n",
        "        no_sources_found = (verification_result.get(\"sources_found\", 0) == 0)\n",
        "\n",
        "        if self.strict_mode and (\"study\" in tl or \"journal\" in tl) and (no_web or no_sources_found):\n",
        "            final_score = min(final_score, 60)\n",
        "\n",
        "        # Verdict\n",
        "        if final_score >= 80: verdict = \"LIKELY CREDIBLE\"\n",
        "        elif final_score >= 60: verdict = \"QUESTIONABLE — VERIFY FURTHER\"\n",
        "        elif final_score >= 40: verdict = \"LIKELY MISLEADING\"\n",
        "        else: verdict = \"LIKELY FAKE NEWS\"\n",
        "\n",
        "        return {\n",
        "            \"final_score\": final_score,\n",
        "            \"verdict\": verdict,\n",
        "            \"expert_score\": expert_score,\n",
        "            \"expert_flags\": expert_flags,\n",
        "            \"ai_analysis\": {\n",
        "                \"sentiment\": sentiment,\n",
        "                \"writing_quality\": writing_quality\n",
        "            },\n",
        "            \"web_verification\": verification_result\n",
        "        }\n",
        "\n",
        "# Reinitialize the detector to apply changes\n",
        "try:\n",
        "    detector = HoaxDetector()\n",
        "    print(\"Detector ready\")\n",
        "except NameError:\n",
        "    detector = HoaxDetector()\n",
        "    print(\"Detector created\")"
      ],
      "metadata": {
        "id": "6c-w6Z51bLzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da4c0a2-b3a9-4582-e046-6c7d49fe9ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 300 online clickbait words\n",
            "Loading AI models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analyzer loaded\n",
            "Detector ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Markdown For GUI**"
      ],
      "metadata": {
        "id": "q8n9AsyGbXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def result_to_markdown(text, res):\n",
        "    def bullet_list(items):\n",
        "        if not items: return \"None\"\n",
        "        return \"\\n\".join([f\"- {it}\" for it in items])\n",
        "\n",
        "    sent = res[\"ai_analysis\"][\"sentiment\"]\n",
        "    wq   = res[\"ai_analysis\"][\"writing_quality\"]\n",
        "    web  = res[\"web_verification\"] or {}\n",
        "\n",
        "    # Top sources list\n",
        "    sources_md = \"None\"\n",
        "    srcs = web.get(\"sources\", [])[:3]\n",
        "    if srcs:\n",
        "        lines = []\n",
        "        for i, s in enumerate(srcs, 1):\n",
        "            badge = \" (reputable)\" if s.get(\"is_reputable\") else \"\"\n",
        "            url = s.get(\"url\",\"\")\n",
        "            title = s.get(\"title\",\"(title unavailable)\")\n",
        "            snip = s.get(\"snippet\",\"\")\n",
        "            lines.append(f\"**{i}. [{title}]({url})**{badge}\\n   {snip}\")\n",
        "        sources_md = \"\\n\\n\".join(lines)\n",
        "\n",
        "    web_note = \"\"\n",
        "    if web.get(\"note\"):\n",
        "        web_note = f\"\\n> ℹ{web['note']}\"\n",
        "\n",
        "    md = f\"\"\"### Input\n",
        "{text.strip()[:800] + ('…' if len(text.strip())>800 else '')}\n",
        "\n",
        "---\n",
        "\n",
        "### Final Assessment\n",
        "- **Final Credibility Score:** **{res['final_score']}/100**\n",
        "- **Verdict:** {res['verdict']}\n",
        "\n",
        "### Expert System\n",
        "- **Expert Score:** {res['expert_score']}/100\n",
        "- **Flags:**\n",
        "{bullet_list(res['expert_flags'])}\n",
        "\n",
        "### AI Analysis\n",
        "- **Sentiment:** {sent.get('label','')} ({sent.get('score',0):.2f})\n",
        "- **Avg Sentence Length:** {wq['avg_sentence_length']:.1f} words\n",
        "- **Spelling Quality:** {wq['spelling_score']:.1f}/100\n",
        "- **Subjectivity:** {wq['subjectivity']:.2f}\n",
        "\n",
        "### Web Verification\n",
        "- **Sources Found:** {web.get('sources_found',0)}\n",
        "- **Reputable Sources:** {web.get('reputable_sources',0)}\n",
        "- **Credibility Boost:** +{web.get('credibility_boost',0)}\n",
        "{web_note}\n",
        "\n",
        "**Top Sources:**\n",
        "{sources_md}\n",
        "\"\"\"\n",
        "    return md\n"
      ],
      "metadata": {
        "id": "laj8kZ08ba-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradio GUI**"
      ],
      "metadata": {
        "id": "yCdbpppHbpNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "\n",
        "def _write_df_to_tempfile(df: pd.DataFrame) -> str:\n",
        "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
        "    df.to_csv(tmp.name, index=False, encoding=\"utf-8\")\n",
        "    return tmp.name\n",
        "\n",
        "def _analyze_list(claims):\n",
        "    rows = []\n",
        "    for t in claims:\n",
        "        try:\n",
        "            res = detector.analyze(t, quiet=True)\n",
        "            wq  = res[\"ai_analysis\"][\"writing_quality\"]\n",
        "            web = res[\"web_verification\"] or {}\n",
        "            rows.append({\n",
        "                \"text\": t[:120] + (\"…\" if len(t) > 120 else \"\"),\n",
        "                \"final_score\": res[\"final_score\"],\n",
        "                \"verdict\": res[\"verdict\"],\n",
        "                \"expert_score\": res[\"expert_score\"],\n",
        "                \"subjectivity\": round(wq.get(\"subjectivity\", 0.0), 3),\n",
        "                \"spelling\": round(wq.get(\"spelling_score\", 0.0), 1),\n",
        "                \"reputable_sources\": web.get(\"reputable_sources\", 0),\n",
        "            })\n",
        "        except Exception as e:\n",
        "            rows.append({\n",
        "                \"text\": t[:120] + (\"…\" if len(t) > 120 else \"\"),\n",
        "                \"final_score\": None,\n",
        "                \"verdict\": f\"Error: {e}\",\n",
        "                \"expert_score\": None,\n",
        "                \"subjectivity\": None,\n",
        "                \"spelling\": None,\n",
        "                \"reputable_sources\": None,\n",
        "            })\n",
        "    df = pd.DataFrame(rows, columns=[\n",
        "        \"text\",\"final_score\",\"verdict\",\"expert_score\",\"subjectivity\",\"spelling\",\"reputable_sources\"\n",
        "    ])\n",
        "    csv_path = _write_df_to_tempfile(df)\n",
        "    return df, csv_path\n",
        "\n",
        "def analyze_one(text):\n",
        "    if not text or not text.strip():\n",
        "        return \"Please paste a headline or paragraph.\"\n",
        "    res = detector.analyze(text, quiet=True)\n",
        "    return result_to_markdown(text, res)\n",
        "\n",
        "def analyze_batch_multiline(big_text):\n",
        "    claims = [ln.strip() for ln in (big_text or \"\").splitlines() if ln.strip()]\n",
        "    if not claims:\n",
        "        df = pd.DataFrame(columns=[\"text\",\"final_score\",\"verdict\",\"expert_score\",\"subjectivity\",\"spelling\",\"reputable_sources\"])\n",
        "        return df, _write_df_to_tempfile(df)\n",
        "    return _analyze_list(claims)\n",
        "\n",
        "def analyze_batch_file(file_path: str):\n",
        "    if not file_path:\n",
        "        df = pd.DataFrame(columns=[\"text\",\"final_score\",\"verdict\",\"expert_score\",\"subjectivity\",\"spelling\",\"reputable_sources\"])\n",
        "        return df, _write_df_to_tempfile(df)\n",
        "\n",
        "    name = file_path.lower()\n",
        "    try:\n",
        "        if name.endswith(\".txt\"):\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                content = f.read()\n",
        "            claims = [ln.strip() for ln in content.splitlines() if ln.strip()]\n",
        "\n",
        "        elif name.endswith(\".csv\"):\n",
        "            df_in = pd.read_csv(file_path)\n",
        "            claims = df_in.iloc[:, 0].dropna().astype(str).tolist()\n",
        "\n",
        "        elif name.endswith((\".xls\", \".xlsx\")):\n",
        "            df_in = pd.read_excel(file_path)\n",
        "            claims = df_in.iloc[:, 0].dropna().astype(str).tolist()\n",
        "\n",
        "        elif name.endswith(\".ods\"):\n",
        "            try:\n",
        "                df_in = pd.read_excel(file_path, engine=\"odf\")\n",
        "                claims = df_in.iloc[:, 0].dropna().astype(str).tolist()\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(\"Reading .ods requires 'odfpy'. Install with: !pip install odfpy\") from e\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file type. Please upload .txt, .csv, .xlsx, .xls, or .ods.\")\n",
        "    except Exception as e:\n",
        "        df = pd.DataFrame([{\"text\": f\"Error reading file: {e}\"}])\n",
        "        return df, _write_df_to_tempfile(df)\n",
        "\n",
        "    if not claims:\n",
        "        df = pd.DataFrame(columns=[\"text\",\"final_score\",\"verdict\",\"expert_score\",\"subjectivity\",\"spelling\",\"reputable_sources\"])\n",
        "        return df, _write_df_to_tempfile(df)\n",
        "\n",
        "    return _analyze_list(claims)\n",
        "\n",
        "with gr.Blocks(title=\"Hoax Detector\") as demo:\n",
        "\n",
        "    with gr.Tab(\"Single Check\"):\n",
        "        ta = gr.TextArea(label=\"Paste a headline or paragraph\", lines=6, placeholder=\"Paste text here…\")\n",
        "        btn = gr.Button(\"Analyze\")\n",
        "        out = gr.Markdown()\n",
        "        btn.click(fn=analyze_one, inputs=ta, outputs=out)\n",
        "\n",
        "    with gr.Tab(\"Batch Check (Multi-line)\"):\n",
        "        ta_batch = gr.TextArea(label=\"One claim per line\", lines=10, placeholder=\"Line 1: …\\nLine 2: …\\nLine 3: …\")\n",
        "        btn_b = gr.Button(\"Analyze Batch\")\n",
        "        df_out = gr.Dataframe(label=\"Results\", interactive=False)\n",
        "        dl = gr.File(label=\"Download CSV\", interactive=False)\n",
        "        btn_b.click(fn=analyze_batch_multiline, inputs=ta_batch, outputs=[df_out, dl])\n",
        "\n",
        "    with gr.Tab(\"Batch Check (Upload File)\"):\n",
        "        fi = gr.File(\n",
        "            label=\"Upload .txt, .csv, .xlsx, .xls, or .ods file (first column = claims)\",\n",
        "            file_types=[\".txt\", \".csv\", \".xlsx\", \".xls\", \".ods\"],\n",
        "            type=\"filepath\"\n",
        "        )\n",
        "        btn_f = gr.Button(\"Analyze File\")\n",
        "        df_out_f = gr.Dataframe(label=\"Results\", interactive=False)\n",
        "        dl_f = gr.File(label=\"Download CSV\", interactive=False)\n",
        "        btn_f.click(fn=analyze_batch_file, inputs=fi, outputs=[df_out_f, dl_f])\n",
        "\n",
        "demo.queue().launch(debug=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "iWC2AvdVbrvD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "1e85b13a-cb45-4c41-ed3f-212a915805e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a6bf7b55968500ace5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a6bf7b55968500ace5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detector.analyze(\"Sun orbits the Earth\")\n"
      ],
      "metadata": {
        "id": "sTcmFohte-zW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3955637c-5de7-45e2-cf2f-94a91c9ead4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'final_score': 35,\n",
              " 'verdict': 'LIKELY FAKE NEWS',\n",
              " 'expert_score': 40,\n",
              " 'expert_flags': ['Contradicts established scientific consensus',\n",
              "  'No explicit sources or citations found'],\n",
              " 'ai_analysis': {'sentiment': {'label': 'POSITIVE',\n",
              "   'score': 0.9980733394622803},\n",
              "  'writing_quality': {'avg_sentence_length': 4.0,\n",
              "   'spelling_score': 50.0,\n",
              "   'subjectivity': 0.0}},\n",
              " 'web_verification': {'sources_found': 5,\n",
              "  'reputable_sources': 0,\n",
              "  'credibility_boost': 5,\n",
              "  'sources': [{'title': \"Earth's orbit\",\n",
              "    'url': 'https://en.wikipedia.org/wiki/Earth%27s_orbit',\n",
              "    'snippet': 'Earth orbits the Sun at an average distance of 149.60 million km (92.96 million mi), or 8.317 light-minutes, [1] in a counterclockwise direction.',\n",
              "    'is_reputable': False},\n",
              "   {'title': 'Our Sun: Facts',\n",
              "    'url': 'https://science.nasa.gov/sun/facts/',\n",
              "    'snippet': 'At its equator, the Sun completes one rotation in 25 Earth days. At its poles, the Sun rotates once on its axis every 36 Earth days. Measuring a “day” ...',\n",
              "    'is_reputable': False},\n",
              "   {'title': 'When did we realize Earth orbits the Sun?',\n",
              "    'url': 'https://www.astronomy.com/science/when-did-we-realize-that-the-earth-orbits-the-sun/',\n",
              "    'snippet': 'The idea that Earth orbits the Sun is ancient. Around 230 B.C., the Greek philosopher Aristarchus suggested that this was the case.',\n",
              "    'is_reputable': False},\n",
              "   {'title': 'Why Does The Earth Orbit Around The Sun?',\n",
              "    'url': 'https://www.youtube.com/watch?v=IXxQNvXDd-Q',\n",
              "    'snippet': \"The Earth is held in orbit by the Sun's gravity. The sun's gravitational force is strongest at its Center therefore planets that are closest to the Sun will be pulled in tight\",\n",
              "    'is_reputable': False},\n",
              "   {'title': 'The Earth Is Not Revolving Around The Sun But Something ...',\n",
              "    'url': 'https://www.ndtv.com/science/the-earth-is-not-revolving-around-the-sun-but-something-else-nearby-nasa-explains-what-it-is-6116873',\n",
              "    'snippet': 'New insights into planetary motion reveal that Earth is not technically orbiting the Sun. Instead, both the Sun and Earth revolve around a ...',\n",
              "    'is_reputable': False}]}}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}